{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING FOR IMAGE CLASSIFICATION OF COSTA RICAN DISHES USING RESNET50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Image Classification with Transfer Learning using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# transform for the train data\n",
    "# Define transformations for training data:\n",
    "# 1. RandomResizedCrop: Randomly crops and resizes images to 224x224 for consistency and data augmentation.\n",
    "# 2. RandomHorizontalFlip: Horizontally flips images randomly for augmentation.\n",
    "# 3. RandomRotation: Randomly rotates the images in a 15 degrees angle.\n",
    "# 4. ColorJitter: Adjusts brightness and contrast for variety in image appearance.\n",
    "# 5. ToTensor: Converts images to PyTorch tensors, the format required for model input.\n",
    "# 6. Normalize: Normalizes images to have a specific mean and standard deviation, aligning with pre-trained model expectations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=25),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# transform for the validation data\n",
    "# Define transformations for validation data:\n",
    "# 1. Resize: Increases the size of the image to 256x256 pixels.\n",
    "# 2. CenterCrop: Crops the center part of the image to 224x224, ensuring it's the same size as the training images.\n",
    "# 3. ToTensor: Converts the image to a PyTorch tensor, suitable for model input.\n",
    "# 4. Normalize: Normalizes the image with the specified mean and standard deviation, matching the training data's normalization.\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load the datasets\n",
    "path_traning_data = \"../dataset/trainig-data\"\n",
    "path_validation_data = \"../dataset/test-data\"\n",
    "train_dataset = ImageFolder(root=path_traning_data, transform=train_transforms)\n",
    "validation_dataset = ImageFolder(root=path_validation_data, transform=validation_transforms)\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# load the pre-trained ResNet50 model\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Get the number of input features for the final fully connected layer of the pre-trained model\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Replace the final fully connected layer with a new one tailored to the number of classes in the dataset\n",
    "# This is necessary to adapt the pre-trained model for the specific classification task\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "\n",
    "# move the model to a GPU if it is available\n",
    "# Check and set the device for Apple M1 (MPS)\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define loss function and optimizer with increased weight decay for L2 regularization\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# set numbers of epochs\n",
    "num_epochs= 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig phase and validation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "\n",
    "    # Iterate over training data\n",
    "    for images, labels in train_loader:\n",
    "        # Move images and labels to the device (GPU or CPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Reset gradients for new iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute model output\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate loss between output and true labels\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagate the error and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss over the batch\n",
    "        training_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    training_loss = training_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    validation_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # No gradient update during validation to reduce memory usage\n",
    "    with torch.no_grad():\n",
    "        # Iterate over validation data\n",
    "        for images, labels in validation_loader:\n",
    "\n",
    "            # Move images and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Compute model output\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Calculate the number of correct predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for this epoch\n",
    "    validation_loss = validation_loss / len(validation_loader.dataset)\n",
    "    validation_accuracy = correct_predictions / len(validation_loader.dataset)\n",
    "\n",
    "    # Store losses and accuracy for plotting\n",
    "    training_losses.append(training_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Training Loss: {training_loss:.4f}, Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Adjust grid line style for transparency\n",
    "mpl.rcParams['grid.alpha'] = 0.03  # Adjust grid transparency\n",
    "\n",
    "# Define themed colors\n",
    "training_loss_color = \"#008080\"\n",
    "validation_loss_color = \"#C0C0C0\"\n",
    "validation_accuracy_color = \"#C0C0C0\"\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.lineplot(x=range(num_epochs), y=training_losses, label='Training Loss', linewidth=3.5)\n",
    "sns.lineplot(x=range(num_epochs), y=validation_losses, label='Validation Loss', linewidth=3.5)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models-CNN/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ResNet50 model for Dish Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet50 model\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Get the number of input features in the final fully connected (fc) layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Replace the final fc layer with a new layer with outputs matching the number of classes in the dataset\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "\n",
    "# Load the model's saved weights\n",
    "model.load_state_dict(torch.load('models-CNN/model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Prepare the image\n",
    "# Define a function to preprocess the image\n",
    "def process_image(image_path):\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Open and transform the image\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0) # Add a batch dimension\n",
    "    return image\n",
    "\n",
    "# Make a prediction\n",
    "idx2label = {0: 'arroz-con-pollo', 1: 'chifrijo', 2: 'gallo-pinto', 3: 'tamales'}\n",
    "# Define a function to make a prediction on an image\n",
    "def predict_image(image_path):\n",
    "    # Process the image\n",
    "    image = process_image(image_path)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_class = idx2label[predicted.item()]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example of using the prediction function\n",
    "image_path = 'your_image_path'\n",
    "#image_path = '../dataset/dishes_example/IMG_20230610_172940.jpg'\n",
    "predicted_class = predict_image(image_path)\n",
    "print(f'Predicted class for Costa Rica dish: {predicted_class}')\n",
    "\n",
    "# Save the predicted dish in a txt file\n",
    "text_to_write = predicted_class\n",
    "with open('predicted-dish.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
